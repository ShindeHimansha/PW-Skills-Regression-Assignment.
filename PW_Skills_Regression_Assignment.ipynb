{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is Simple Linear Regression\n",
        "-\tSimple Linear Regression is a statistical method that models the relationship between one independent variable (X) and one dependent variable (Y) using a linear equation.\n",
        "-\tIt attempts to find the best-fitting straight line through the data points that minimizes the differences between observed and predicted values.\n",
        "-\tThe method is foundational in predictive modeling and is used to understand how changes in the predictor variable relate to changes in the response variable.\n",
        "#2. What are the key assumptions of Simple Linear Regression\n",
        "-\tThere is a linear relationship between the independent and dependent variables, meaning the relationship can be represented by a straight line.\n",
        "-\tThe residuals (errors) are normally distributed with a mean of zero.\n",
        "-\tThe residuals have constant variance at every level of X (homoscedasticity).\n",
        "-\tThe observations are independent of each other.\n",
        "#3. What does the coefficient m represent in the equation Y=mX+c\n",
        "-\tThe coefficient m represents the slope of the regression line.\n",
        "-\tIt indicates the change in the dependent variable Y for a one-unit change in the independent variable X.\n",
        "-\tThe sign of m indicates the direction of the relationship: positive means Y increases as X increases, negative means Y decreases as X increases.\n",
        "#4. What does the intercept c represent in the equation Y=mX+c\n",
        "-\tThe intercept c represents the value of the dependent variable Y when the independent variable X equals zero.\n",
        "-\tIt is the point where the regression line crosses the Y-axis on a graph.\n",
        "-\tIn some contexts, the intercept may not have a meaningful interpretation, especially if X=0 is outside the range of observed data.\n",
        "#5. How do we calculate the slope m in Simple Linear Regression\n",
        "-\tThe slope m is calculated using the formula: m = Σ[(xi - x̄)(yi - ȳ)] / Σ[(xi - x̄)²], where x̄ and ȳ are the means of x and y respectively.\n",
        "-\tThis formula minimizes the sum of squared residuals between observed and predicted values.\n",
        "-\tThe calculation can also be expressed as m = cov(X,Y) / var(X), relating the covariance of X and Y to the variance of X.\n",
        "#6. What is the purpose of the least squares method in Simple Linear Regression\n",
        "-\tThe least squares method aims to find the line that minimizes the sum of squared differences between observed and predicted values.\n",
        "-\tIt provides a mathematical criterion for determining the \"best-fitting\" line through the data points.\n",
        "-\tThis method ensures that the resulting regression model is optimal in terms of minimizing prediction errors.\n",
        "#7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        "-\tR² represents the proportion of variance in the dependent variable that is explained by the independent variable.\n",
        "-\tIt ranges from 0 to 1, where 0 indicates no explanatory power and 1 indicates perfect prediction.\n",
        "-\tAn R² of 0.7 means that 70% of the variation in Y can be explained by X, while the remaining 30% is unexplained.\n",
        "#8. What is Multiple Linear Regression\n",
        "-\tMultiple Linear Regression is an extension of Simple Linear Regression that models the relationship between multiple independent variables and one dependent variable.\n",
        "-\tThe model takes the form Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ + ε, where β values are coefficients and ε is the error term.\n",
        "-\tIt allows for examining how multiple factors simultaneously affect the outcome variable.\n",
        "#9. What is the main difference between Simple and Multiple Linear Regression\n",
        "-\tSimple Linear Regression uses only one independent variable to predict the dependent variable, while Multiple Linear Regression uses two or more independent variables.\n",
        "-\tMultiple Linear Regression can capture more complex relationships and account for confounding variables.\n",
        "-\tThe interpretation of coefficients in Multiple Linear Regression is more nuanced, as each coefficient represents the effect of one variable while holding others constant.\n",
        "#10. What are the key assumptions of Multiple Linear Regression\n",
        "-\tLinearity: The relationship between independent variables and the dependent variable is linear.\n",
        "-\tIndependence: The residuals are independent of each other.\n",
        "-\tHomoscedasticity: The residuals have constant variance across all levels of the independent variables.\n",
        "-\tMulticollinearity: The independent variables are not highly correlated with each other.\n",
        "#11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "-\tHeteroscedasticity occurs when the variance of residuals varies across different levels of independent variables.\n",
        "-\tIt can lead to inefficient parameter estimates and invalid standard errors, making hypothesis tests unreliable.\n",
        "-\tModels with heteroscedasticity may still be unbiased but are no longer the best linear unbiased estimators (BLUE).\n",
        "#12. How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "-\tRemove one or more of the highly correlated variables from the model.\n",
        "-\tCombine the correlated variables into a single predictor using dimension reduction techniques like Principal Component Analysis.\n",
        "-\tUse regularization methods such as Ridge Regression or Lasso Regression to constrain coefficient values.\n",
        "#13. What are some common techniques for transforming categorical variables for use in regression models\n",
        "-\tOne-hot encoding: Creating binary (0/1) dummy variables for each category, excluding one reference category.\n",
        "-\tEffect coding: Similar to one-hot encoding but using -1 for the reference category instead of 0.\n",
        "-\tTarget encoding: Replacing categorical values with the mean of the target variable for each category.\n",
        "#14. What is the role of interaction terms in Multiple Linear Regression\n",
        "-\tInteraction terms capture the combined effect of two or more independent variables that is different from their individual effects.\n",
        "-\tThey are represented as products of variables (e.g., X₁ × X₂) in the regression equation.\n",
        "-\tIncluding interaction terms allows the model to account for situations where the effect of one variable depends on the value of another variable.\n",
        "#15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "-\tIn Simple Linear Regression, the intercept is the expected value of Y when X equals zero.\n",
        "-\tIn Multiple Linear Regression, the intercept is the expected value of Y when all independent variables equal zero.\n",
        "-\tThe intercept in Multiple Linear Regression is often less interpretable because a scenario where all predictors equal zero may be unrealistic or outside the observed data range.\n",
        "#16. What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "-\tThe slope coefficients quantify the relationship between each independent variable and the dependent variable.\n",
        "-\tThey determine how much the predicted value changes when an independent variable increases by one unit.\n",
        "-\tLarger absolute values of slopes indicate stronger effects, which leads to more dramatic changes in predictions as the independent variables change.\n",
        "#17. How does the intercept in a regression model provide context for the relationship between variables\n",
        "-\tThe intercept establishes a baseline value for the dependent variable when all independent variables are zero.\n",
        "-\tIt anchors the regression line or hyperplane in the coordinate system.\n",
        "-\tThe intercept can reveal information about underlying processes when X=0 has a meaningful interpretation in the context of the problem.\n",
        "#18. What are the limitations of using R² as a sole measure of model performance\n",
        "-\tR² always increases when additional variables are added to the model, even if they don't significantly improve predictions.\n",
        "-\tIt doesn't account for model complexity or potential overfitting.\n",
        "-\tR² doesn't directly indicate prediction accuracy or whether the model assumptions are met.\n",
        "#19. How would you interpret a large standard error for a regression coefficient\n",
        "-\tA large standard error indicates high uncertainty in the estimate of the coefficient.\n",
        "-\tIt suggests that the coefficient value could vary substantially across different samples from the same population.\n",
        "-\tLarge standard errors may result from small sample sizes, high multicollinearity, or heteroscedasticity.\n",
        "#20. How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "-\tHeteroscedasticity appears as a funnel, fan, or other non-random pattern in plots of residuals versus predicted values.\n",
        "-\tIt can be detected using statistical tests like the Breusch-Pagan test or White test.\n",
        "-\tAddressing heteroscedasticity is important because it violates a key assumption of linear regression and can lead to invalid standard errors and confidence intervals.\n",
        "#21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
        "-\tThis indicates that the model includes many variables that don't significantly contribute to explaining the dependent variable.\n",
        "-\tThe high R² is artificially inflated by the inclusion of these additional variables.\n",
        "-\tThe low adjusted R² reveals that the model is likely overfitted and would perform poorly on new data.\n",
        "#22. Why is it important to scale variables in Multiple Linear Regression\n",
        "-\tScaling puts variables on similar scales, making their coefficients directly comparable in terms of relative importance.\n",
        "-\tIt improves numerical stability in the calculation of regression coefficients.\n",
        "-\tScaling can help address convergence issues in iterative algorithms used for regression with regularization.\n",
        "#23. What is polynomial regression\n",
        "-\tPolynomial regression is a form of regression analysis where the relationship between the independent variable and dependent variable is modeled as an nth degree polynomial.\n",
        "-\tIt extends the linear model by adding polynomial terms (X², X³, etc.) as predictors.\n",
        "-\tPolynomial regression can capture non-linear relationships that simple linear regression cannot model.\n",
        "#24. How does polynomial regression differ from linear regression\n",
        "-\tPolynomial regression can model curved relationships, while linear regression only captures straight-line relationships.\n",
        "-\tThe equation for polynomial regression includes higher powers of the independent variables.\n",
        "-\tDespite including non-linear terms, polynomial regression is still considered a form of linear regression because it's linear in the parameters.\n",
        "#25. When is polynomial regression used\n",
        "-\tPolynomial regression is used when there is evidence of a non-linear relationship between variables.\n",
        "-\tIt's appropriate when a scatter plot of the data shows a clear curved pattern.\n",
        "-\tIt's useful when theoretical knowledge suggests that the relationship follows a polynomial form.\n",
        "#26. What is the general equation for polynomial regression\n",
        "-\tThe general equation for polynomial regression is Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βₙXⁿ + ε.\n",
        "-\tEach term represents a different power of the independent variable X.\n",
        "-\tThe coefficients β determine the shape and direction of the polynomial curve.\n",
        "#27. Can polynomial regression be applied to multiple variables\n",
        "-\tYes, polynomial regression can be applied to multiple variables, creating what's known as a multivariate polynomial regression.\n",
        "-\tThis includes not only powers of individual variables but also cross-products (interaction terms).\n",
        "-\tThe full model can quickly become complex with many parameters as the number of variables and degree increase.\n",
        "#28. What are the limitations of polynomial regression\n",
        "-\tPolynomial regression is prone to overfitting, especially with higher-degree polynomials.\n",
        "-\tIt can be highly sensitive to outliers, particularly at the boundaries of the data.\n",
        "-\tExtrapolation beyond the range of observed data can lead to wildly inaccurate predictions.\n",
        "#29. What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "-\tCross-validation techniques to assess prediction performance on unseen data.\n",
        "-\tInformation criteria like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) that balance model fit and complexity.\n",
        "-\tStatistical significance tests for the highest-order term to determine if it contributes meaningfully to the model.\n",
        "#30. Why is visualization important in polynomial regression\n",
        "-\tVisualization helps identify the appropriate polynomial degree by showing how well different models fit the data pattern.\n",
        "-\tIt can reveal issues like overfitting where the model follows noise rather than the underlying trend.\n",
        "-\tVisual inspection of residuals can identify regions where the model fits poorly or assumptions are violated.\n",
        "#31. How is polynomial regression implemented in Python\n",
        "-\tPolynomial features can be created using scikit-learn's PolynomialFeatures transformer.\n",
        "-\tThe transformed features are then passed to a standard linear regression model.\n",
        "-\tPython libraries like NumPy and statsmodels also provide functions for polynomial curve fitting and regression analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "c4JWldBXORHI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zFtDUuZN9Tt"
      },
      "outputs": [],
      "source": []
    }
  ]
}